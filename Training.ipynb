{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"qnetwork_11act_boltzmann\"\n",
    "episodes = 1300\n",
    "samples_per_target = 200\n",
    "targets_per_episode = 5\n",
    "batch_size = 16\n",
    "batch_per_episode = 100\n",
    "\n",
    "action_size = 11\n",
    "agent = QLearningAgentBoltzmann(action_size=action_size, n=100, gamma=0.95,\n",
    "                        temperature=0.03, temperature_decay=0.99998, temperature_min=0.003,\n",
    "                        learning_rate=0.00005 * batch_size, warmup_steps=3000, learning_rate_decay=0.99997,\n",
    "                        stored_episodes=15, samples_per_episode=targets_per_episode * samples_per_target)\n",
    "\n",
    "best_reward = -1000000\n",
    "for e in range(episodes):\n",
    "    # Reset the environment (PT1 system)\n",
    "    pt1_with_delay = ControlSystem.PT1(K=2, T=5, delta_t=0.1, delay=2)\n",
    "    total_reward = 0\n",
    "    state = torch.tensor([pt1_with_delay.y_prev], device=device)\n",
    "\n",
    "    for j in range(targets_per_episode):\n",
    "        target = torch.rand((1,), device=device) * 2\n",
    "        for k in range(samples_per_target):\n",
    "            action = agent.act(state, target)\n",
    "\n",
    "            control_signal = action / (action_size - 1)\n",
    "\n",
    "            output = pt1_with_delay.calculate(control_signal)\n",
    "            next_state = output\n",
    "\n",
    "            # Calculate reward (negative of the absolute error)\n",
    "            reward = -torch.abs(next_state - target)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store the experience\n",
    "            agent.remember(state, action, reward, target)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Replay experience\n",
    "    for _ in range(batch_per_episode):\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        agent.save(f\"{model_name}_{e}.pth\")\n",
    "\n",
    "agent.save(f\"{model_name}.pth\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"qnetwork_11act\"\n",
    "episodes = 2000\n",
    "samples_per_target = 200\n",
    "targets_per_episode = 5\n",
    "batch_size = 16\n",
    "batch_per_episode = 100\n",
    "\n",
    "action_size = 11\n",
    "agent = QLearningAgent(action_size=action_size, n=100, gamma=0.95,\n",
    "                        epsilon=0.3, epsilon_decay=0.99999, epsilon_min=0.08,\n",
    "                        learning_rate=0.00005 * batch_size, warmup_steps=3000, learning_rate_decay=0.99998,\n",
    "                        stored_episodes=15, samples_per_episode=targets_per_episode * samples_per_target)\n",
    "\n",
    "\n",
    "best_reward = -1000000\n",
    "for e in range(episodes):\n",
    "    # Reset the environment (PT1 system)\n",
    "    pt1_with_delay = ControlSystem.PT1(K=2, T=5, delta_t=0.1, delay=2)\n",
    "    total_reward = 0\n",
    "    state = torch.tensor([pt1_with_delay.y_prev], device=device)\n",
    "\n",
    "    for j in range(targets_per_episode):\n",
    "        target = torch.rand((1,), device=device) * 2\n",
    "        for k in range(samples_per_target):\n",
    "            action = agent.act(state, target)\n",
    "\n",
    "            control_signal = action / (action_size - 1)\n",
    "\n",
    "            output = pt1_with_delay.calculate(control_signal)\n",
    "            next_state = output\n",
    "\n",
    "            # Calculate reward (negative of the absolute error)\n",
    "            reward = -torch.abs(next_state - target)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store the experience\n",
    "            agent.remember(state, action, reward, target)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Replay experience\n",
    "    for _ in range(batch_per_episode):\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        agent.save(f\"{model_name}_{e}.pth\")\n",
    "\n",
    "agent.save(f\"{model_name}.pth\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_name = \"qnetwork_soft\"\n",
    "\n",
    "episodes = 1300\n",
    "samples_per_episode = 200\n",
    "targets_per_episode = 4\n",
    "batch_size = 16\n",
    "batch_per_episode = 100\n",
    "\n",
    "agent = QLearningAgentSoft(action_search_batch=32,\n",
    "                                 gamma=0.988,\n",
    "                                 temperature=0.016,\n",
    "                                 average_weight=0.5,\n",
    "                                 learning_rate=0.00003 * batch_size, warmup_steps=1000,\n",
    "                                 learning_rate_decay=0.99998)\n",
    "\n",
    "for e in range(episodes):\n",
    "    # Reset the environment (PT1 system)\n",
    "    pt1_with_delay = ControlSystem.PT1(K=2, T=5, delta_t=0.1, delay=2)\n",
    "    total_reward = 0\n",
    "    state = torch.tensor([pt1_with_delay.y_prev], device=device)\n",
    "\n",
    "    for j in range(targets_per_episode):\n",
    "        target = torch.tensor([random.uniform(0, 2)], device=device)\n",
    "        for k in range(samples_per_episode):\n",
    "            control_signal, u, s = agent.act(state, target)\n",
    "            next_state = pt1_with_delay.calculate(control_signal)\n",
    "\n",
    "            # Calculate reward (negative of the absolute error)\n",
    "            reward = -torch.abs(next_state - target)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store the experience\n",
    "            agent.remember(reward, next_state, target)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Replay experience\n",
    "    for _ in range(batch_per_episode):\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        agent.save(f\"{model_name}_{e}.pth\")\n",
    "\n",
    "agent.save(f\"{model_name}.pth\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "action_size = 11\n",
    "\n",
    "K = 2\n",
    "T = 5\n",
    "delay = 2\n",
    "delta_t = 0.1\n",
    "\n",
    "simulation_time = 30\n",
    "steps = int(simulation_time / delta_t)\n",
    "time_values = torch.linspace(0.0, simulation_time, steps)\n",
    "setpoints = [0.2, 0.5, 0.7, 1.2, 1.5, 1.9]\n",
    "rows, cols = 3, 2\n",
    "\n",
    "agent = QLearningAgent(action_size=action_size, n=100, gamma=0.988,\n",
    "                        epsilon=0.00, epsilon_decay=0.99999, epsilon_min=0.08,\n",
    "                        #temperature=0.02, temperature_decay=0.99999, temperature_min=0.08,\n",
    "                        learning_rate=0.0, warmup_steps=0, learning_rate_decay=0.0,\n",
    "                        stored_episodes=1, samples_per_episode=steps)\n",
    "\n",
    "\n",
    "best = \"\"\n",
    "best_error = torch.tensor((10000.0,), device=device)\n",
    "for path in Path(\"./\").glob(\"qnetwork_11act*.pth\"):\n",
    "    overall_mean_error = 0.0\n",
    "    agent.load(path)\n",
    "    for setpoint in setpoints:\n",
    "        pt1_with_delay = ControlSystem.PT1(K=K, T=T, delta_t=delta_t, delay=delay)\n",
    "        state = pt1_with_delay.y_prev\n",
    "        setpoint = torch.tensor([setpoint], device=device)\n",
    "        output_values = torch.zeros([steps], device=device)\n",
    "        control_values = torch.zeros([steps], device=device)\n",
    "\n",
    "        for step in range(steps):\n",
    "            action = agent.act(state, setpoint)\n",
    "\n",
    "            control_signal = action / (action_size - 1)\n",
    "\n",
    "            output = pt1_with_delay.calculate(control_signal)\n",
    "            next_state = output\n",
    "\n",
    "            reward = -torch.abs(next_state - setpoint)\n",
    "            agent.remember(state, action, reward, setpoint)\n",
    "            output_values[step] = output\n",
    "            control_values[step] = control_signal\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        mean_error = torch.mean(torch.abs(output_values - setpoint))\n",
    "        overall_mean_error += mean_error / len(setpoints)\n",
    "\n",
    "    if overall_mean_error < best_error:\n",
    "        best = path\n",
    "        best_error = overall_mean_error\n",
    "\n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
