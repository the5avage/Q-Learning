{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2000, Total Reward: tensor([-576.6440], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 51/2000, Total Reward: tensor([-551.4729], device='cuda:0')\n",
      "Episode 101/2000, Total Reward: tensor([-465.9258], device='cuda:0')\n",
      "Episode 151/2000, Total Reward: tensor([-566.2161], device='cuda:0')\n",
      "Episode 201/2000, Total Reward: tensor([-618.2343], device='cuda:0')\n",
      "Episode 251/2000, Total Reward: tensor([-569.6149], device='cuda:0')\n",
      "Episode 301/2000, Total Reward: tensor([-476.7028], device='cuda:0')\n",
      "Episode 351/2000, Total Reward: tensor([-122.7113], device='cuda:0')\n",
      "Episode 401/2000, Total Reward: tensor([-373.1590], device='cuda:0')\n",
      "Episode 451/2000, Total Reward: tensor([-217.6268], device='cuda:0')\n",
      "Episode 501/2000, Total Reward: tensor([-210.8472], device='cuda:0')\n",
      "Episode 551/2000, Total Reward: tensor([-364.1454], device='cuda:0')\n",
      "Episode 601/2000, Total Reward: tensor([-214.4722], device='cuda:0')\n",
      "Episode 651/2000, Total Reward: tensor([-291.5023], device='cuda:0')\n",
      "Episode 701/2000, Total Reward: tensor([-237.0614], device='cuda:0')\n",
      "Episode 751/2000, Total Reward: tensor([-108.2357], device='cuda:0')\n",
      "Episode 801/2000, Total Reward: tensor([-256.6436], device='cuda:0')\n",
      "Episode 851/2000, Total Reward: tensor([-260.1833], device='cuda:0')\n",
      "Episode 901/2000, Total Reward: tensor([-251.2161], device='cuda:0')\n",
      "Episode 951/2000, Total Reward: tensor([-275.3584], device='cuda:0')\n",
      "Episode 1001/2000, Total Reward: tensor([-288.1904], device='cuda:0')\n",
      "Episode 1051/2000, Total Reward: tensor([-220.5173], device='cuda:0')\n",
      "Episode 1101/2000, Total Reward: tensor([-187.9109], device='cuda:0')\n",
      "Episode 1151/2000, Total Reward: tensor([-249.3768], device='cuda:0')\n",
      "Episode 1201/2000, Total Reward: tensor([-460.2069], device='cuda:0')\n",
      "Episode 1251/2000, Total Reward: tensor([-287.3598], device='cuda:0')\n",
      "Episode 1301/2000, Total Reward: tensor([-206.1316], device='cuda:0')\n",
      "Episode 1351/2000, Total Reward: tensor([-131.6293], device='cuda:0')\n",
      "Episode 1401/2000, Total Reward: tensor([-178.2556], device='cuda:0')\n",
      "Episode 1451/2000, Total Reward: tensor([-180.5870], device='cuda:0')\n",
      "Episode 1501/2000, Total Reward: tensor([-260.5831], device='cuda:0')\n",
      "Episode 1551/2000, Total Reward: tensor([-315.4094], device='cuda:0')\n",
      "Episode 1601/2000, Total Reward: tensor([-161.7889], device='cuda:0')\n",
      "Episode 1651/2000, Total Reward: tensor([-152.4290], device='cuda:0')\n",
      "Episode 1701/2000, Total Reward: tensor([-141.7132], device='cuda:0')\n",
      "Episode 1751/2000, Total Reward: tensor([-162.8695], device='cuda:0')\n",
      "Episode 1801/2000, Total Reward: tensor([-123.7787], device='cuda:0')\n",
      "Episode 1851/2000, Total Reward: tensor([-376.9445], device='cuda:0')\n",
      "Episode 1901/2000, Total Reward: tensor([-129.3298], device='cuda:0')\n",
      "Episode 1951/2000, Total Reward: tensor([-251.9463], device='cuda:0')\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"qnetwork_11act\"\n",
    "episodes = 2000\n",
    "samples_per_target = 200\n",
    "targets_per_episode = 5\n",
    "batch_size = 16\n",
    "batch_per_episode = 100\n",
    "\n",
    "action_size = 11\n",
    "agent = QLearningAgent(action_size=action_size, n=100, gamma=0.95,\n",
    "                        epsilon=0.3, epsilon_decay=0.99999, epsilon_min=0.08,\n",
    "                        learning_rate=0.00005 * batch_size, warmup_steps=3000, learning_rate_decay=0.99998,\n",
    "                        stored_episodes=15, samples_per_episode=targets_per_episode * samples_per_target)\n",
    "\n",
    "\n",
    "best_reward = -1000000\n",
    "for e in range(episodes):\n",
    "    # Reset the environment (PT1 system)\n",
    "    pt1_with_delay = ControlSystem.PT1(K=2, T=5, delta_t=0.1, delay=2)\n",
    "    total_reward = 0\n",
    "    state = torch.tensor([pt1_with_delay.y_prev], device=device)\n",
    "\n",
    "    for j in range(targets_per_episode):\n",
    "        target = torch.rand((1,), device=device) * 2\n",
    "        for k in range(samples_per_target):\n",
    "            action = agent.act(state, target)\n",
    "\n",
    "            control_signal = action / (action_size - 1)\n",
    "\n",
    "            output = pt1_with_delay.calculate(control_signal)\n",
    "            next_state = output\n",
    "\n",
    "            # Calculate reward (negative of the absolute error)\n",
    "            reward = -torch.abs(next_state - target)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store the experience\n",
    "            agent.remember(state, action, reward, target)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Replay experience\n",
    "    for _ in range(batch_per_episode):\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        agent.save(f\"{model_name}_{e}.pth\")\n",
    "\n",
    "agent.save(f\"{model_name}.pth\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2150, Total Reward: tensor([-565.0029], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 51/2150, Total Reward: tensor([-314.5203], device='cuda:0')\n",
      "Episode 101/2150, Total Reward: tensor([-748.6987], device='cuda:0')\n",
      "Episode 151/2150, Total Reward: tensor([-1072.7100], device='cuda:0')\n",
      "Episode 201/2150, Total Reward: tensor([-125.3066], device='cuda:0')\n",
      "Episode 251/2150, Total Reward: tensor([-760.4337], device='cuda:0')\n",
      "Episode 301/2150, Total Reward: tensor([-289.3593], device='cuda:0')\n",
      "Episode 351/2150, Total Reward: tensor([-330.4626], device='cuda:0')\n",
      "Episode 401/2150, Total Reward: tensor([-249.3636], device='cuda:0')\n",
      "Episode 451/2150, Total Reward: tensor([-229.8311], device='cuda:0')\n",
      "Episode 501/2150, Total Reward: tensor([-264.6424], device='cuda:0')\n",
      "Episode 551/2150, Total Reward: tensor([-186.3651], device='cuda:0')\n",
      "Episode 601/2150, Total Reward: tensor([-249.7781], device='cuda:0')\n",
      "Episode 651/2150, Total Reward: tensor([-356.2170], device='cuda:0')\n",
      "Episode 701/2150, Total Reward: tensor([-142.2096], device='cuda:0')\n",
      "Episode 751/2150, Total Reward: tensor([-260.2078], device='cuda:0')\n",
      "Episode 801/2150, Total Reward: tensor([-223.2927], device='cuda:0')\n",
      "Episode 851/2150, Total Reward: tensor([-201.8748], device='cuda:0')\n",
      "Episode 901/2150, Total Reward: tensor([-281.9551], device='cuda:0')\n",
      "Episode 951/2150, Total Reward: tensor([-320.9726], device='cuda:0')\n",
      "Episode 1001/2150, Total Reward: tensor([-294.9081], device='cuda:0')\n",
      "Episode 1051/2150, Total Reward: tensor([-261.1740], device='cuda:0')\n",
      "Episode 1101/2150, Total Reward: tensor([-227.7665], device='cuda:0')\n",
      "Episode 1151/2150, Total Reward: tensor([-388.8783], device='cuda:0')\n",
      "Episode 1201/2150, Total Reward: tensor([-344.5721], device='cuda:0')\n",
      "Episode 1251/2150, Total Reward: tensor([-265.2656], device='cuda:0')\n",
      "Episode 1301/2150, Total Reward: tensor([-376.3461], device='cuda:0')\n",
      "Episode 1351/2150, Total Reward: tensor([-410.4384], device='cuda:0')\n",
      "Episode 1401/2150, Total Reward: tensor([-249.8020], device='cuda:0')\n",
      "Episode 1451/2150, Total Reward: tensor([-353.0347], device='cuda:0')\n",
      "Episode 1501/2150, Total Reward: tensor([-183.4786], device='cuda:0')\n",
      "Episode 1551/2150, Total Reward: tensor([-214.3748], device='cuda:0')\n",
      "Episode 1601/2150, Total Reward: tensor([-283.5132], device='cuda:0')\n",
      "Episode 1651/2150, Total Reward: tensor([-270.3600], device='cuda:0')\n",
      "Episode 1701/2150, Total Reward: tensor([-249.6688], device='cuda:0')\n",
      "Episode 1751/2150, Total Reward: tensor([-216.4916], device='cuda:0')\n",
      "Episode 1801/2150, Total Reward: tensor([-106.5414], device='cuda:0')\n",
      "Episode 1851/2150, Total Reward: tensor([-254.9709], device='cuda:0')\n",
      "Episode 1901/2150, Total Reward: tensor([-291.7299], device='cuda:0')\n",
      "Episode 1951/2150, Total Reward: tensor([-238.4115], device='cuda:0')\n",
      "Episode 2001/2150, Total Reward: tensor([-176.2162], device='cuda:0')\n",
      "Episode 2051/2150, Total Reward: tensor([-170.0244], device='cuda:0')\n",
      "Episode 2101/2150, Total Reward: tensor([-223.2086], device='cuda:0')\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"qnetwork_11act_boltzmann\"\n",
    "episodes = 1300\n",
    "samples_per_target = 200\n",
    "targets_per_episode = 5\n",
    "batch_size = 16\n",
    "batch_per_episode = 100\n",
    "\n",
    "action_size = 11\n",
    "agent = QLearningAgentBoltzmann(action_size=action_size, n=100, gamma=0.95,\n",
    "                        temperature=0.03, temperature_decay=0.99998, temperature_min=0.003,\n",
    "                        learning_rate=0.00005 * batch_size, warmup_steps=3000, learning_rate_decay=0.99997,\n",
    "                        stored_episodes=15, samples_per_episode=targets_per_episode * samples_per_target)\n",
    "\n",
    "best_reward = -1000000\n",
    "for e in range(episodes):\n",
    "    # Reset the environment (PT1 system)\n",
    "    pt1_with_delay = ControlSystem.PT1(K=2, T=5, delta_t=0.1, delay=2)\n",
    "    total_reward = 0\n",
    "    state = torch.tensor([pt1_with_delay.y_prev], device=device)\n",
    "\n",
    "    for j in range(targets_per_episode):\n",
    "        target = torch.rand((1,), device=device) * 2\n",
    "        for k in range(samples_per_target):\n",
    "            action = agent.act(state, target)\n",
    "\n",
    "            control_signal = action / (action_size - 1)\n",
    "\n",
    "            output = pt1_with_delay.calculate(control_signal)\n",
    "            next_state = output\n",
    "\n",
    "            # Calculate reward (negative of the absolute error)\n",
    "            reward = -torch.abs(next_state - target)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store the experience\n",
    "            agent.remember(state, action, reward, target)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Replay experience\n",
    "    for _ in range(batch_per_episode):\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        agent.save(f\"{model_name}_{e}.pth\")\n",
    "\n",
    "agent.save(f\"{model_name}.pth\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Episode 1/5000, Total Reward: tensor([-536.7311], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 51/5000, Total Reward: tensor([-649.6022], device='cuda:0')\n",
      "Episode 101/5000, Total Reward: tensor([-505.0512], device='cuda:0')\n",
      "Episode 151/5000, Total Reward: tensor([-411.9357], device='cuda:0')\n",
      "Episode 201/5000, Total Reward: tensor([-290.7041], device='cuda:0')\n",
      "Episode 251/5000, Total Reward: tensor([-725.0445], device='cuda:0')\n",
      "Episode 301/5000, Total Reward: tensor([-311.7230], device='cuda:0')\n",
      "Episode 351/5000, Total Reward: tensor([-272.1707], device='cuda:0')\n",
      "Episode 401/5000, Total Reward: tensor([-477.9097], device='cuda:0')\n",
      "Episode 451/5000, Total Reward: tensor([-131.3875], device='cuda:0')\n",
      "Episode 501/5000, Total Reward: tensor([-441.6857], device='cuda:0')\n",
      "Episode 551/5000, Total Reward: tensor([-187.0427], device='cuda:0')\n",
      "Episode 601/5000, Total Reward: tensor([-255.3547], device='cuda:0')\n",
      "Episode 651/5000, Total Reward: tensor([-118.3151], device='cuda:0')\n",
      "Episode 701/5000, Total Reward: tensor([-169.3453], device='cuda:0')\n",
      "Episode 751/5000, Total Reward: tensor([-183.8093], device='cuda:0')\n",
      "Episode 801/5000, Total Reward: tensor([-93.1951], device='cuda:0')\n",
      "Episode 851/5000, Total Reward: tensor([-115.4334], device='cuda:0')\n",
      "Episode 901/5000, Total Reward: tensor([-223.7686], device='cuda:0')\n",
      "Episode 951/5000, Total Reward: tensor([-78.0526], device='cuda:0')\n",
      "Episode 1001/5000, Total Reward: tensor([-103.7986], device='cuda:0')\n",
      "Episode 1051/5000, Total Reward: tensor([-141.5629], device='cuda:0')\n",
      "Episode 1101/5000, Total Reward: tensor([-195.4936], device='cuda:0')\n",
      "Episode 1151/5000, Total Reward: tensor([-189.4482], device='cuda:0')\n",
      "Episode 1201/5000, Total Reward: tensor([-157.4429], device='cuda:0')\n",
      "Episode 1251/5000, Total Reward: tensor([-156.4929], device='cuda:0')\n",
      "Episode 1301/5000, Total Reward: tensor([-189.6144], device='cuda:0')\n",
      "Episode 1351/5000, Total Reward: tensor([-219.3540], device='cuda:0')\n",
      "Episode 1401/5000, Total Reward: tensor([-132.9896], device='cuda:0')\n",
      "Episode 1451/5000, Total Reward: tensor([-312.5593], device='cuda:0')\n",
      "Episode 1501/5000, Total Reward: tensor([-58.6413], device='cuda:0')\n",
      "Episode 1551/5000, Total Reward: tensor([-159.8553], device='cuda:0')\n",
      "Episode 1601/5000, Total Reward: tensor([-104.1818], device='cuda:0')\n",
      "Episode 1651/5000, Total Reward: tensor([-141.0593], device='cuda:0')\n",
      "Episode 1701/5000, Total Reward: tensor([-121.6870], device='cuda:0')\n",
      "Episode 1751/5000, Total Reward: tensor([-171.2083], device='cuda:0')\n",
      "Episode 1801/5000, Total Reward: tensor([-120.8342], device='cuda:0')\n",
      "Episode 1851/5000, Total Reward: tensor([-188.8648], device='cuda:0')\n",
      "Episode 1901/5000, Total Reward: tensor([-327.7433], device='cuda:0')\n",
      "Episode 1951/5000, Total Reward: tensor([-193.5241], device='cuda:0')\n",
      "Episode 2001/5000, Total Reward: tensor([-175.9817], device='cuda:0')\n",
      "Episode 2051/5000, Total Reward: tensor([-231.5890], device='cuda:0')\n",
      "Episode 2101/5000, Total Reward: tensor([-103.2673], device='cuda:0')\n",
      "Episode 2151/5000, Total Reward: tensor([-190.0205], device='cuda:0')\n",
      "Episode 2201/5000, Total Reward: tensor([-154.4624], device='cuda:0')\n",
      "Episode 2251/5000, Total Reward: tensor([-79.6978], device='cuda:0')\n",
      "Episode 2301/5000, Total Reward: tensor([-231.8836], device='cuda:0')\n",
      "Episode 2351/5000, Total Reward: tensor([-254.1455], device='cuda:0')\n",
      "Episode 2401/5000, Total Reward: tensor([-244.8541], device='cuda:0')\n",
      "Episode 2451/5000, Total Reward: tensor([-130.6145], device='cuda:0')\n",
      "Episode 2501/5000, Total Reward: tensor([-404.4040], device='cuda:0')\n",
      "Episode 2551/5000, Total Reward: tensor([-237.9669], device='cuda:0')\n",
      "Episode 2601/5000, Total Reward: tensor([-163.3197], device='cuda:0')\n",
      "Episode 2651/5000, Total Reward: tensor([-222.8701], device='cuda:0')\n",
      "Episode 2701/5000, Total Reward: tensor([-223.8806], device='cuda:0')\n",
      "Episode 2751/5000, Total Reward: tensor([-194.4136], device='cuda:0')\n",
      "Episode 2801/5000, Total Reward: tensor([-175.2069], device='cuda:0')\n",
      "Episode 2851/5000, Total Reward: tensor([-67.5422], device='cuda:0')\n",
      "Episode 2901/5000, Total Reward: tensor([-236.3376], device='cuda:0')\n",
      "Episode 2951/5000, Total Reward: tensor([-257.9256], device='cuda:0')\n",
      "Episode 3001/5000, Total Reward: tensor([-171.4205], device='cuda:0')\n",
      "Episode 3051/5000, Total Reward: tensor([-282.2998], device='cuda:0')\n",
      "Episode 3101/5000, Total Reward: tensor([-174.1018], device='cuda:0')\n",
      "Episode 3151/5000, Total Reward: tensor([-268.3015], device='cuda:0')\n",
      "Episode 3201/5000, Total Reward: tensor([-246.2021], device='cuda:0')\n",
      "Episode 3251/5000, Total Reward: tensor([-170.3994], device='cuda:0')\n",
      "Episode 3301/5000, Total Reward: tensor([-266.7296], device='cuda:0')\n",
      "Episode 3351/5000, Total Reward: tensor([-244.6755], device='cuda:0')\n",
      "Episode 3401/5000, Total Reward: tensor([-225.7796], device='cuda:0')\n",
      "Episode 3451/5000, Total Reward: tensor([-303.3040], device='cuda:0')\n",
      "Episode 3501/5000, Total Reward: tensor([-172.2521], device='cuda:0')\n",
      "Episode 3551/5000, Total Reward: tensor([-185.3893], device='cuda:0')\n",
      "Episode 3601/5000, Total Reward: tensor([-231.3331], device='cuda:0')\n",
      "Episode 3651/5000, Total Reward: tensor([-280.1444], device='cuda:0')\n",
      "Episode 3701/5000, Total Reward: tensor([-234.5715], device='cuda:0')\n",
      "Episode 3751/5000, Total Reward: tensor([-108.6966], device='cuda:0')\n",
      "Episode 3801/5000, Total Reward: tensor([-243.7639], device='cuda:0')\n",
      "Episode 3851/5000, Total Reward: tensor([-194.0954], device='cuda:0')\n",
      "Episode 3901/5000, Total Reward: tensor([-275.6156], device='cuda:0')\n",
      "Episode 3951/5000, Total Reward: tensor([-162.3917], device='cuda:0')\n",
      "Episode 4001/5000, Total Reward: tensor([-187.0323], device='cuda:0')\n",
      "Episode 4051/5000, Total Reward: tensor([-216.0222], device='cuda:0')\n",
      "Episode 4101/5000, Total Reward: tensor([-285.1729], device='cuda:0')\n",
      "Episode 4151/5000, Total Reward: tensor([-244.5332], device='cuda:0')\n",
      "Episode 4201/5000, Total Reward: tensor([-127.8397], device='cuda:0')\n",
      "Episode 4251/5000, Total Reward: tensor([-143.0446], device='cuda:0')\n",
      "Episode 4301/5000, Total Reward: tensor([-83.6196], device='cuda:0')\n",
      "Episode 4351/5000, Total Reward: tensor([-139.5244], device='cuda:0')\n",
      "Episode 4401/5000, Total Reward: tensor([-154.4650], device='cuda:0')\n",
      "Episode 4451/5000, Total Reward: tensor([-166.7347], device='cuda:0')\n",
      "Episode 4501/5000, Total Reward: tensor([-207.3518], device='cuda:0')\n",
      "Episode 4551/5000, Total Reward: tensor([-361.9626], device='cuda:0')\n",
      "Episode 4601/5000, Total Reward: tensor([-292.3924], device='cuda:0')\n",
      "Episode 4651/5000, Total Reward: tensor([-254.0884], device='cuda:0')\n",
      "Episode 4701/5000, Total Reward: tensor([-272.5109], device='cuda:0')\n",
      "Episode 4751/5000, Total Reward: tensor([-295.0128], device='cuda:0')\n",
      "Episode 4801/5000, Total Reward: tensor([-193.8671], device='cuda:0')\n",
      "Episode 4851/5000, Total Reward: tensor([-228.4086], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m         total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;66;03m# Store the experience\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremember\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m         state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Replay experience\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saruman\\ML\\QLearnTemperatureControl\\QNetwork.py:300\u001b[0m, in \u001b[0;36mQLearningAgentSoft.remember\u001b[1;34m(self, reward, next_state, target)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremember\u001b[39m(\u001b[39mself\u001b[39m, reward, next_state, target):\n\u001b[0;32m    298\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mappend((\n\u001b[0;32m    299\u001b[0m         torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpast_states), device\u001b[39m=\u001b[39mdevice),\n\u001b[1;32m--> 300\u001b[0m         torch\u001b[39m.\u001b[39;49mtensor(\u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpast_actions), device\u001b[39m=\u001b[39;49mdevice),\n\u001b[0;32m    301\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpast_action_distribution,\n\u001b[0;32m    302\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpast_entropy,\n\u001b[0;32m    303\u001b[0m         next_state,\n\u001b[0;32m    304\u001b[0m         reward,\n\u001b[0;32m    305\u001b[0m         target))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_name = \"qnetwork_soft\"\n",
    "\n",
    "episodes = 1300\n",
    "samples_per_episode = 200\n",
    "targets_per_episode = 4\n",
    "batch_size = 16\n",
    "batch_per_episode = 100\n",
    "\n",
    "agent = QLearningAgentSoft(action_search_batch=32,\n",
    "                                 gamma=0.988,\n",
    "                                 temperature=0.016,\n",
    "                                 average_weight=0.5,\n",
    "                                 learning_rate=0.00003 * batch_size, warmup_steps=1000,\n",
    "                                 learning_rate_decay=0.99998)\n",
    "\n",
    "for e in range(episodes):\n",
    "    # Reset the environment (PT1 system)\n",
    "    pt1_with_delay = ControlSystem.PT1(K=2, T=5, delta_t=0.1, delay=2)\n",
    "    total_reward = 0\n",
    "    state = torch.tensor([pt1_with_delay.y_prev], device=device)\n",
    "\n",
    "    for j in range(targets_per_episode):\n",
    "        target = torch.tensor([random.uniform(0, 2)], device=device)\n",
    "        for k in range(samples_per_episode):\n",
    "            control_signal, u, s = agent.act(state, target)\n",
    "            next_state = pt1_with_delay.calculate(control_signal)\n",
    "\n",
    "            # Calculate reward (negative of the absolute error)\n",
    "            reward = -torch.abs(next_state - target)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store the experience\n",
    "            agent.remember(reward, next_state, target)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Replay experience\n",
    "    for _ in range(batch_per_episode):\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        agent.save(f\"{model_name}_{e}.pth\")\n",
    "\n",
    "agent.save(f\"{model_name}.pth\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"qnetwork_11act_comp\"\n",
    "episodes = 1200\n",
    "samples_per_target = 200\n",
    "targets_per_episode = 5\n",
    "batch_size = 16\n",
    "batch_per_episode = 100\n",
    "\n",
    "action_size = 11\n",
    "agent = QLearningAgentComp(action_size=action_size, n=100, gamma=0.95,\n",
    "                        epsilon=0.3, epsilon_decay=0.99999, epsilon_min=0.08,\n",
    "                        learning_rate=0.00005 * batch_size, warmup_steps=3000, learning_rate_decay=0.99998,\n",
    "                        stored_episodes=15, samples_per_episode=targets_per_episode * samples_per_target)\n",
    "\n",
    "\n",
    "best_reward = -1000000\n",
    "for e in range(episodes):\n",
    "    # Reset the environment (PT1 system)\n",
    "    pt1_with_delay = ControlSystem.PT1(K=2, T=5, delta_t=0.1, delay=2)\n",
    "    total_reward = 0\n",
    "    state = torch.tensor([pt1_with_delay.y_prev], device=device)\n",
    "\n",
    "    for j in range(targets_per_episode):\n",
    "        target = torch.rand((1,), device=device) * 2\n",
    "        for k in range(samples_per_target):\n",
    "            action = agent.act(state, target)\n",
    "\n",
    "            control_signal = action / (action_size - 1)\n",
    "\n",
    "            output = pt1_with_delay.calculate(control_signal)\n",
    "            next_state = output\n",
    "\n",
    "            # Calculate reward (negative of the absolute error)\n",
    "            reward = -torch.abs(next_state - target)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store the experience\n",
    "            agent.remember(state, action, reward, target)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Replay experience\n",
    "    for _ in range(batch_per_episode):\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "        agent.save(f\"{model_name}_{e}.pth\")\n",
    "\n",
    "agent.save(f\"{model_name}.pth\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qnetwork_11act_boltzmann3_1250.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import ControlSystem\n",
    "from QNetwork import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "action_size = 11\n",
    "\n",
    "K = 2\n",
    "T = 5\n",
    "delay = 2\n",
    "delta_t = 0.1\n",
    "\n",
    "simulation_time = 30\n",
    "steps = int(simulation_time / delta_t)\n",
    "time_values = torch.linspace(0.0, simulation_time, steps)\n",
    "setpoints = [0.2, 0.5, 0.7, 1.2, 1.5, 1.9]\n",
    "rows, cols = 3, 2\n",
    "\n",
    "agent = QLearningAgent(action_size=action_size, n=100, gamma=0.988,\n",
    "                        epsilon=0.00, epsilon_decay=0.99999, epsilon_min=0.08,\n",
    "                        #temperature=0.02, temperature_decay=0.99999, temperature_min=0.08,\n",
    "                        learning_rate=0.0, warmup_steps=0, learning_rate_decay=0.0,\n",
    "                        stored_episodes=1, samples_per_episode=steps)\n",
    "\n",
    "\n",
    "best = \"\"\n",
    "best_error = torch.tensor((10000.0,), device=device)\n",
    "for path in Path(\"./\").glob(\"qnetwork_11act*.pth\"):\n",
    "    overall_mean_error = 0.0\n",
    "    agent.load(path)\n",
    "    for setpoint in setpoints:\n",
    "        pt1_with_delay = ControlSystem.PT1(K=K, T=T, delta_t=delta_t, delay=delay)\n",
    "        state = pt1_with_delay.y_prev\n",
    "        setpoint = torch.tensor([setpoint], device=device)\n",
    "        output_values = torch.zeros([steps], device=device)\n",
    "        control_values = torch.zeros([steps], device=device)\n",
    "\n",
    "        for step in range(steps):\n",
    "            action = agent.act(state, setpoint)\n",
    "\n",
    "            control_signal = action / (action_size - 1)\n",
    "\n",
    "            output = pt1_with_delay.calculate(control_signal)\n",
    "            next_state = output\n",
    "\n",
    "            reward = -torch.abs(next_state - setpoint)\n",
    "            agent.remember(state, action, reward, setpoint)\n",
    "            output_values[step] = output\n",
    "            control_values[step] = control_signal\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        mean_error = torch.mean(torch.abs(output_values - setpoint))\n",
    "        overall_mean_error += mean_error / len(setpoints)\n",
    "\n",
    "    if overall_mean_error < best_error:\n",
    "        best = path\n",
    "        best_error = overall_mean_error\n",
    "\n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
